

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Optimizers API Reference &mdash; nevergrad  documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Parametrization API reference" href="parametrization_ref.html" />
    <link rel="prev" title="Examples - Nevergrad for machine learning" href="machinelearning.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> nevergrad
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Getting started</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimization.html">How to perform optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="parametrization.html">Parametrizing your optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="machinelearning.html">Examples - Nevergrad for machine learning</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Optimizers API Reference</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#optimizer-api">Optimizer API</a></li>
<li class="toctree-l2"><a class="reference internal" href="#callbacks">Callbacks</a></li>
<li class="toctree-l2"><a class="reference internal" href="#configurable-optimizers">Configurable optimizers</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optimizers">Optimizers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="parametrization_ref.html">Parametrization API reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarking.html">Running algorithm benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="r.html">Examples - Nevergrad for R</a></li>
<li class="toctree-l1"><a class="reference internal" href="benchmarks.html">Examples of benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="pyomo.html">Examples - Working with Pyomo model</a></li>
<li class="toctree-l1"><a class="reference internal" href="windows.html">Installation and configuration on Windows</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing to Nevergrad</a></li>
<li class="toctree-l1"><a class="reference internal" href="opencompetition2020.html">Open Optimization Competition 2020</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">nevergrad</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Optimizers API Reference</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/optimizers_ref.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="optimizers-api-reference">
<h1>Optimizers API Reference<a class="headerlink" href="#optimizers-api-reference" title="Permalink to this headline">¶</a></h1>
<div class="section" id="optimizer-api">
<h2>Optimizer API<a class="headerlink" href="#optimizer-api" title="Permalink to this headline">¶</a></h2>
<p>All the optimizers share the following common API:</p>
<dl class="py class">
<dt id="nevergrad.optimizers.base.Optimizer">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimizers.base.</code><code class="sig-name descname">Optimizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimizers.base.Optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Algorithm framework with 3 main functions:</p>
<ul class="simple">
<li><p><code class="code docutils literal notranslate"><span class="pre">ask()</span></code> which provides a candidate on which to evaluate the function to optimize.</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">tell(candidate,</span> <span class="pre">loss)</span></code> which lets you provide the loss associated to points.</p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">provide_recommendation()</span></code> which provides the best final candidate.</p></li>
</ul>
<p>Typically, one would call <code class="code docutils literal notranslate"><span class="pre">ask()</span></code> num_workers times, evaluate the
function on these num_workers points in parallel, update with the fitness value when the
evaluations is finished, and iterate until the budget is over. At the very end,
one would call provide_recommendation for the estimated optimum.</p>
<p>This class is abstract, it provides internal equivalents for the 3 main functions,
among which at least <code class="code docutils literal notranslate"><span class="pre">_internal_ask_candidate</span></code> has to be overridden.</p>
<p>Each optimizer instance should be used only once, with the initial provided budget</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>parametrization</strong> (<em>int</em><em> or </em><a class="reference internal" href="parametrization_ref.html#nevergrad.p.Parameter" title="nevergrad.p.Parameter"><em>Parameter</em></a>) – either the dimension of the optimization space, or its parametrization</p></li>
<li><p><strong>budget</strong> (<em>int/None</em>) – number of allowed evaluations</p></li>
<li><p><strong>num_workers</strong> (<em>int</em>) – number of evaluations which will be run in parallel at once</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt id="nevergrad.optimizers.base.Optimizer.ask">
<code class="sig-name descname">ask</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; nevergrad.parametrization.core.Parameter<a class="headerlink" href="#nevergrad.optimizers.base.Optimizer.ask" title="Permalink to this definition">¶</a></dt>
<dd><p>Provides a point to explore.
This function can be called multiple times to explore several points in parallel</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The candidate to try on the objective function. <code class="code docutils literal notranslate"><span class="pre">p.Parameter</span></code> have field <code class="code docutils literal notranslate"><span class="pre">args</span></code> and <code class="code docutils literal notranslate"><span class="pre">kwargs</span></code>
which can be directly used on the function (<code class="code docutils literal notranslate"><span class="pre">objective_function(*candidate.args,</span> <span class="pre">**candidate.kwargs)</span></code>).</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="parametrization_ref.html#nevergrad.p.Parameter" title="nevergrad.p.Parameter">p.Parameter</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nevergrad.optimizers.base.Optimizer.dimension">
<em class="property">property </em><code class="sig-name descname">dimension</code><a class="headerlink" href="#nevergrad.optimizers.base.Optimizer.dimension" title="Permalink to this definition">¶</a></dt>
<dd><p>Dimension of the optimization space.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nevergrad.optimizers.base.Optimizer.dump">
<code class="sig-name descname">dump</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">filepath</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>pathlib.Path<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#nevergrad.optimizers.base.Optimizer.dump" title="Permalink to this definition">¶</a></dt>
<dd><p>Pickles the optimizer into a file.</p>
</dd></dl>

<dl class="py method">
<dt id="nevergrad.optimizers.base.Optimizer.load">
<em class="property">classmethod </em><code class="sig-name descname">load</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">filepath</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>pathlib.Path<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; X<a class="headerlink" href="#nevergrad.optimizers.base.Optimizer.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a pickle and checks that the class is correct.</p>
</dd></dl>

<dl class="py method">
<dt id="nevergrad.optimizers.base.Optimizer.minimize">
<code class="sig-name descname">minimize</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">objective_function</span><span class="p">:</span> <span class="n">Callable<span class="p">[</span><span class="p">[</span><span class="p">…</span><span class="p">]</span><span class="p">, </span>Union<span class="p">[</span>float<span class="p">, </span>Tuple<span class="p">[</span>float<span class="p">, </span><span class="p">…</span><span class="p">]</span><span class="p">, </span>List<span class="p">[</span>float<span class="p">]</span><span class="p">, </span>numpy.ndarray<span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">executor</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>nevergrad.common.typing.ExecutorLike<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">batch_mode</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">verbosity</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">0</span></em><span class="sig-paren">)</span> &#x2192; nevergrad.parametrization.core.Parameter<a class="headerlink" href="#nevergrad.optimizers.base.Optimizer.minimize" title="Permalink to this definition">¶</a></dt>
<dd><p>Optimization (minimization) procedure</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>objective_function</strong> (<em>callable</em>) – A callable to optimize (minimize)</p></li>
<li><p><strong>executor</strong> (<em>Executor</em>) – An executor object, with method <code class="code docutils literal notranslate"><span class="pre">submit(callable,</span> <span class="pre">*args,</span> <span class="pre">**kwargs)</span></code> and returning a Future-like object
with methods <code class="code docutils literal notranslate"><span class="pre">done()</span> <span class="pre">-&gt;</span> <span class="pre">bool</span></code> and <code class="code docutils literal notranslate"><span class="pre">result()</span> <span class="pre">-&gt;</span> <span class="pre">float</span></code>. The executor role is to dispatch the execution of
the jobs locally/on a cluster/with multithreading depending on the implementation.
Eg: <code class="code docutils literal notranslate"><span class="pre">concurrent.futures.ThreadPoolExecutor</span></code></p></li>
<li><p><strong>batch_mode</strong> (<em>bool</em>) – when <code class="code docutils literal notranslate"><span class="pre">num_workers</span> <span class="pre">=</span> <span class="pre">n</span> <span class="pre">&gt;</span> <span class="pre">1</span></code>, whether jobs are executed by batch (<code class="code docutils literal notranslate"><span class="pre">n</span></code> function evaluations are launched,
we wait for all results and relaunch n evals) or not (whenever an evaluation is finished, we launch
another one)</p></li>
<li><p><strong>verbosity</strong> (<em>int</em>) – print information about the optimization (0: None, 1: fitness values, 2: fitness values and recommendation)</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>The candidate with minimal value. <code class="code docutils literal notranslate"><span class="pre">ng.p.Parameters</span></code> have field <code class="code docutils literal notranslate"><span class="pre">args</span></code> and <code class="code docutils literal notranslate"><span class="pre">kwargs</span></code> which can
be directly used on the function (<code class="code docutils literal notranslate"><span class="pre">objective_function(*candidate.args,</span> <span class="pre">**candidate.kwargs)</span></code>).</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>ng.p.Parameter</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>for evaluation purpose and with the current implementation, it is better to use batch_mode=True</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="nevergrad.optimizers.base.Optimizer.num_ask">
<em class="property">property </em><code class="sig-name descname">num_ask</code><a class="headerlink" href="#nevergrad.optimizers.base.Optimizer.num_ask" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of time the <cite>ask</cite> method was called.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nevergrad.optimizers.base.Optimizer.num_objectives">
<em class="property">property </em><code class="sig-name descname">num_objectives</code><a class="headerlink" href="#nevergrad.optimizers.base.Optimizer.num_objectives" title="Permalink to this definition">¶</a></dt>
<dd><p>Provides 0 if the number is not known yet, else the number of objectives
to optimize upon.</p>
</dd></dl>

<dl class="py method">
<dt id="nevergrad.optimizers.base.Optimizer.num_tell">
<em class="property">property </em><code class="sig-name descname">num_tell</code><a class="headerlink" href="#nevergrad.optimizers.base.Optimizer.num_tell" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of time the <cite>tell</cite> method was called.</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nevergrad.optimizers.base.Optimizer.num_tell_not_asked">
<em class="property">property </em><code class="sig-name descname">num_tell_not_asked</code><a class="headerlink" href="#nevergrad.optimizers.base.Optimizer.num_tell_not_asked" title="Permalink to this definition">¶</a></dt>
<dd><p>Number of time the <code class="code docutils literal notranslate"><span class="pre">tell</span></code> method was called on candidates that were not asked for by the optimizer
(or were suggested).</p>
<dl class="field-list simple">
<dt class="field-odd">Type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nevergrad.optimizers.base.Optimizer.pareto_front">
<code class="sig-name descname">pareto_front</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">size</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">subset</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'random'</span></em>, <em class="sig-param"><span class="n">subset_tentatives</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">12</span></em><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span>nevergrad.parametrization.core.Parameter<span class="p">]</span><a class="headerlink" href="#nevergrad.optimizers.base.Optimizer.pareto_front" title="Permalink to this definition">¶</a></dt>
<dd><p>Pareto front, as a list of Parameter. The losses can be accessed through
parameter.losses</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>size</strong> (<em>int</em><em> (</em><em>optional</em><em>)</em>) – if provided, selects a subset of the full pareto front with the given maximum size</p></li>
<li><p><strong>subset</strong> (<em>str</em>) – method for selecting the subset (“random, “loss-covering”, “domain-covering”, “hypervolume”)</p></li>
<li><p><strong>subset_tentatives</strong> (<em>int</em>) – number of random tentatives for finding a better subset</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>the list of Parameter of the pareto front</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>During non-multiobjective optimization, this returns the current pessimistic best</p>
</div>
</dd></dl>

<dl class="py method">
<dt id="nevergrad.optimizers.base.Optimizer.provide_recommendation">
<code class="sig-name descname">provide_recommendation</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; nevergrad.parametrization.core.Parameter<a class="headerlink" href="#nevergrad.optimizers.base.Optimizer.provide_recommendation" title="Permalink to this definition">¶</a></dt>
<dd><p>Provides the best point to use as a minimum, given the budget that was used</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The candidate with minimal value. p.Parameters have field <code class="code docutils literal notranslate"><span class="pre">args</span></code> and <code class="code docutils literal notranslate"><span class="pre">kwargs</span></code> which can be directly used
on the function (<code class="code docutils literal notranslate"><span class="pre">objective_function(*candidate.args,</span> <span class="pre">**candidate.kwargs)</span></code>).</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="parametrization_ref.html#nevergrad.p.Parameter" title="nevergrad.p.Parameter">p.Parameter</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nevergrad.optimizers.base.Optimizer.recommend">
<code class="sig-name descname">recommend</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; nevergrad.parametrization.core.Parameter<a class="headerlink" href="#nevergrad.optimizers.base.Optimizer.recommend" title="Permalink to this definition">¶</a></dt>
<dd><p>Provides the best candidate to use as a minimum, given the budget that was used.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The candidate with minimal loss. <code class="code docutils literal notranslate"><span class="pre">p.Parameters</span></code> have field <code class="code docutils literal notranslate"><span class="pre">args</span></code> and <code class="code docutils literal notranslate"><span class="pre">kwargs</span></code> which can be directly used
on the function (<code class="code docutils literal notranslate"><span class="pre">objective_function(*candidate.args,</span> <span class="pre">**candidate.kwargs)</span></code>).</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="parametrization_ref.html#nevergrad.p.Parameter" title="nevergrad.p.Parameter">p.Parameter</a></p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nevergrad.optimizers.base.Optimizer.register_callback">
<code class="sig-name descname">register_callback</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">callback</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>Callable<span class="p">[</span><span class="p">[</span>nevergrad.optimization.base.Optimizer<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">, </span>float<span class="p">]</span><span class="p">, </span>None<span class="p">]</span><span class="p">, </span>Callable<span class="p">[</span><span class="p">[</span>nevergrad.optimization.base.Optimizer<span class="p">]</span><span class="p">, </span>None<span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#nevergrad.optimizers.base.Optimizer.register_callback" title="Permalink to this definition">¶</a></dt>
<dd><p>Add a callback method called either when <cite>tell</cite> or <cite>ask</cite> are called, with the same
arguments (including the optimizer / self). This can be useful for custom logging.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>name</strong> (<em>str</em>) – name of the method to register the callback for (either <code class="code docutils literal notranslate"><span class="pre">ask</span></code> or <code class="code docutils literal notranslate"><span class="pre">tell</span></code>)</p></li>
<li><p><strong>callback</strong> (<em>callable</em>) – a callable taking the same parameters as the method it is registered upon (including self)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nevergrad.optimizers.base.Optimizer.remove_all_callbacks">
<code class="sig-name descname">remove_all_callbacks</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#nevergrad.optimizers.base.Optimizer.remove_all_callbacks" title="Permalink to this definition">¶</a></dt>
<dd><p>Removes all registered callables</p>
</dd></dl>

<dl class="py method">
<dt id="nevergrad.optimizers.base.Optimizer.suggest">
<code class="sig-name descname">suggest</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">Any</span></em>, <em class="sig-param"><span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">Any</span></em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#nevergrad.optimizers.base.Optimizer.suggest" title="Permalink to this definition">¶</a></dt>
<dd><p>Suggests a new point to ask.
It will be asked at the next call (last in first out).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>*args</strong> (<em>Any</em>) – positional arguments matching the parametrization pattern.</p></li>
<li><p><strong>*kwargs</strong> (<em>Any</em>) – keyword arguments matching the parametrization pattern.</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>This relies on optmizers implementing a way to deal with unasked candidate.
Some optimizers may not support it and will raise a <code class="code docutils literal notranslate"><span class="pre">TellNotAskedNotSupportedError</span></code>
at <code class="code docutils literal notranslate"><span class="pre">tell</span></code> time.</p></li>
<li><p>LIFO is used so as to be able to suggest and ask straightaway, as an alternative to
creating a new candidate with <code class="code docutils literal notranslate"><span class="pre">optimizer.parametrization.spawn_child(new_value)</span></code></p></li>
</ul>
</div>
</dd></dl>

<dl class="py method">
<dt id="nevergrad.optimizers.base.Optimizer.tell">
<code class="sig-name descname">tell</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">candidate</span><span class="p">:</span> <span class="n">nevergrad.parametrization.core.Parameter</span></em>, <em class="sig-param"><span class="n">loss</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>Tuple<span class="p">[</span>float<span class="p">, </span><span class="p">…</span><span class="p">]</span><span class="p">, </span>List<span class="p">[</span>float<span class="p">]</span><span class="p">, </span>numpy.ndarray<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; None<a class="headerlink" href="#nevergrad.optimizers.base.Optimizer.tell" title="Permalink to this definition">¶</a></dt>
<dd><p>Provides the optimizer with the evaluation of a fitness value for a candidate.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>x</strong> (<em>np.ndarray</em>) – point where the function was evaluated</p></li>
<li><p><strong>loss</strong> (<em>float/list/np.ndarray</em>) – loss of the function (or multi-objective function</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The candidate should generally be one provided by <code class="code docutils literal notranslate"><span class="pre">ask()</span></code>, but can be also
a non-asked candidate. To create a p.Parameter instance from args and kwargs,
you can use <code class="code docutils literal notranslate"><span class="pre">candidate</span> <span class="pre">=</span> <span class="pre">optimizer.parametrization.spawn_child(new_value=your_value)</span></code>:</p>
<ul class="simple">
<li><p>for an <code class="code docutils literal notranslate"><span class="pre">Array(shape(2,))</span></code>: <code class="code docutils literal notranslate"><span class="pre">optimizer.parametrization.spawn_child(new_value=[12,</span> <span class="pre">12])</span></code></p></li>
<li><p>for an <code class="code docutils literal notranslate"><span class="pre">Instrumentation</span></code>: <code class="code docutils literal notranslate"><span class="pre">optimizer.parametrization.spawn_child(new_value=(args,</span> <span class="pre">kwargs))</span></code></p></li>
</ul>
<p>Alternatively, you can provide a suggestion with <code class="code docutils literal notranslate"><span class="pre">optimizer.suggest(*args,</span> <span class="pre">**kwargs)</span></code>, the next <code class="code docutils literal notranslate"><span class="pre">ask</span></code>
will use this suggestion.</p>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="callbacks">
<span id="id1"></span><h2>Callbacks<a class="headerlink" href="#callbacks" title="Permalink to this headline">¶</a></h2>
<p>Callbacks can be registered through the <code class="code docutils literal notranslate"><span class="pre">optimizer.register_callback</span></code> for call on either <code class="code docutils literal notranslate"><span class="pre">ask</span></code> or <code class="code docutils literal notranslate"><span class="pre">tell</span></code> methods. Two of them are available through the
<cite>ng.callbacks</cite> namespace.</p>
<span class="target" id="module-nevergrad.callbacks"></span><dl class="py class">
<dt id="nevergrad.callbacks.EarlyStopping">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.callbacks.</code><code class="sig-name descname">EarlyStopping</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">stopping_criterion</span><span class="p">:</span> <span class="n">Callable<span class="p">[</span><span class="p">[</span>nevergrad.optimization.base.Optimizer<span class="p">]</span><span class="p">, </span>bool<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.callbacks.EarlyStopping" title="Permalink to this definition">¶</a></dt>
<dd><p>Callback for stopping the <code class="code docutils literal notranslate"><span class="pre">minimize</span></code> method before the budget is
fully used.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>stopping_criterion</strong> (<em>func</em><em>(</em><em>optimizer</em><em>) </em><em>-&gt; bool</em>) – function that takes the current optimizer as input and returns True
if the minimization must be stopped</p>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This callback must be register on the “ask” method only.</p>
</div>
<p class="rubric">Example</p>
<p>In the following code, the <code class="code docutils literal notranslate"><span class="pre">minimize</span></code> method will be stopped at the 4th “ask”</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">early_stopping</span> <span class="o">=</span> <span class="n">ng</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="k">lambda</span> <span class="n">opt</span><span class="p">:</span> <span class="n">opt</span><span class="o">.</span><span class="n">num_ask</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span><span class="o">.</span><span class="n">register_callback</span><span class="p">(</span><span class="s2">&quot;ask&quot;</span><span class="p">,</span> <span class="n">early_stopping</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">_func</span><span class="p">,</span> <span class="n">verbosity</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.callbacks.OptimizerDump">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.callbacks.</code><code class="sig-name descname">OptimizerDump</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">filepath</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>pathlib.Path<span class="p">]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.callbacks.OptimizerDump" title="Permalink to this definition">¶</a></dt>
<dd><p>Dumps the optimizer to a pickle file at every call.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>filepath</strong> (<em>str</em><em> or </em><em>Path</em>) – path to the pickle file</p>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.callbacks.ParametersLogger">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.callbacks.</code><code class="sig-name descname">ParametersLogger</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">filepath</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>pathlib.Path<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">append</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">order</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.callbacks.ParametersLogger" title="Permalink to this definition">¶</a></dt>
<dd><p>Logs parameter and run information throughout into a file during
optimization.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>filepath</strong> (<em>str</em><em> or </em><em>pathlib.Path</em>) – the path to dump data to</p></li>
<li><p><strong>append</strong> (<em>bool</em>) – whether to append the file (otherwise it replaces it)</p></li>
<li><p><strong>order</strong> (<em>int</em>) – order of the internal/model parameters to extract</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">logger</span> <span class="o">=</span> <span class="n">ParametersLogger</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">register_callback</span><span class="p">(</span><span class="s2">&quot;tell&quot;</span><span class="p">,</span>  <span class="n">logger</span><span class="p">)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">()</span>
<span class="n">list_of_dict_of_data</span> <span class="o">=</span> <span class="n">logger</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Arrays are converted to lists</p>
</div>
<dl class="py method">
<dt id="nevergrad.callbacks.ParametersLogger.load">
<code class="sig-name descname">load</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span>Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span><a class="headerlink" href="#nevergrad.callbacks.ParametersLogger.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads data from the log file</p>
</dd></dl>

<dl class="py method">
<dt id="nevergrad.callbacks.ParametersLogger.load_flattened">
<code class="sig-name descname">load_flattened</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">max_list_elements</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">24</span></em><span class="sig-paren">)</span> &#x2192; List<span class="p">[</span>Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span><a class="headerlink" href="#nevergrad.callbacks.ParametersLogger.load_flattened" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads data from the log file, and splits lists (arrays) into multiple arguments</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>max_list_elements</strong> (<em>int</em>) – Maximum number of elements displayed from the array, each element is given a
unique id of type list_name#i0_i1_…</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nevergrad.callbacks.ParametersLogger.to_hiplot_experiment">
<code class="sig-name descname">to_hiplot_experiment</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">max_list_elements</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">24</span></em><span class="sig-paren">)</span> &#x2192; Any<a class="headerlink" href="#nevergrad.callbacks.ParametersLogger.to_hiplot_experiment" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts the logs into an hiplot experiment for display.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>max_list_elements</strong> (<em>int</em>) – maximum number of elements of list/arrays to export (only the first elements are extracted)</p>
</dd>
</dl>
<p class="rubric">Example</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">exp</span> <span class="o">=</span> <span class="n">logs</span><span class="o">.</span><span class="n">to_hiplot_experiment</span><span class="p">()</span>
<span class="n">exp</span><span class="o">.</span><span class="n">display</span><span class="p">(</span><span class="n">force_full_width</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>You can easily change the axes of the XY plot:
<code class="code docutils literal notranslate"><span class="pre">exp.display_data(hip.Displays.XY).update({'axis_x':</span> <span class="pre">'0#0',</span> <span class="pre">'axis_y':</span> <span class="pre">'0#1'})</span></code></p></li>
<li><p>For more context about hiplot, check:</p>
<ul>
<li><p>blogpost: <a class="reference external" href="https://ai.facebook.com/blog/hiplot-high-dimensional-interactive-plots-made-easy/">https://ai.facebook.com/blog/hiplot-high-dimensional-interactive-plots-made-easy/</a></p></li>
<li><p>github repo: <a class="reference external" href="https://github.com/facebookresearch/hiplot">https://github.com/facebookresearch/hiplot</a></p></li>
<li><p>documentation: <a class="reference external" href="https://facebookresearch.github.io/hiplot/">https://facebookresearch.github.io/hiplot/</a></p></li>
</ul>
</li>
</ul>
</div>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="nevergrad.callbacks.ProgressBar">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.callbacks.</code><code class="sig-name descname">ProgressBar</code><a class="headerlink" href="#nevergrad.callbacks.ProgressBar" title="Permalink to this definition">¶</a></dt>
<dd><p>Progress bar to register as callback in an optimizer</p>
</dd></dl>

</div>
<div class="section" id="configurable-optimizers">
<h2>Configurable optimizers<a class="headerlink" href="#configurable-optimizers" title="Permalink to this headline">¶</a></h2>
<p>Configurable optimizers share the following API to create optimizers instances:</p>
<dl class="py class">
<dt id="nevergrad.optimizers.base.ConfiguredOptimizer">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimizers.base.</code><code class="sig-name descname">ConfiguredOptimizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">OptimizerClass</span><span class="p">:</span> <span class="n">Type<span class="p">[</span>nevergrad.optimization.base.Optimizer<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">config</span><span class="p">:</span> <span class="n">Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">as_config</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimizers.base.ConfiguredOptimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates optimizer-like instances with configuration.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>OptimizerClass</strong> (<em>type</em>) – class of the optimizer to configure</p></li>
<li><p><strong>config</strong> (<em>dict</em>) – dictionnary of all the configurations</p></li>
<li><p><strong>as_config</strong> (<em>bool</em>) – whether to provide all config as kwargs to the optimizer instantiation (default, see ConfiguredCMA for an example),
or through a config kwarg referencing self. (if True, see EvolutionStrategy for an example)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This provides a default repr which can be bypassed through set_name</p>
</div>
<dl class="py method">
<dt id="nevergrad.optimizers.base.ConfiguredOptimizer.__call__">
<code class="sig-name descname">__call__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span> &#x2192; nevergrad.optimization.base.Optimizer<a class="headerlink" href="#nevergrad.optimizers.base.ConfiguredOptimizer.__call__" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates an optimizer from the parametrization</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>instrumentation</strong> (<em>int</em><em> or </em><a class="reference internal" href="parametrization_ref.html#nevergrad.p.Instrumentation" title="nevergrad.p.Instrumentation"><em>Instrumentation</em></a>) – either the dimension of the optimization space, or its instrumentation</p></li>
<li><p><strong>budget</strong> (<em>int/None</em>) – number of allowed evaluations</p></li>
<li><p><strong>num_workers</strong> (<em>int</em>) – number of evaluations which will be run in parallel at once</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt id="nevergrad.optimizers.base.ConfiguredOptimizer.load">
<code class="sig-name descname">load</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">filepath</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>pathlib.Path<span class="p">]</span></span></em><span class="sig-paren">)</span> &#x2192; nevergrad.optimization.base.Optimizer<a class="headerlink" href="#nevergrad.optimizers.base.ConfiguredOptimizer.load" title="Permalink to this definition">¶</a></dt>
<dd><p>Loads a pickle and checks that it is an Optimizer.</p>
</dd></dl>

<dl class="py method">
<dt id="nevergrad.optimizers.base.ConfiguredOptimizer.set_name">
<code class="sig-name descname">set_name</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">name</span><span class="p">:</span> <span class="n">str</span></em>, <em class="sig-param"><span class="n">register</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span> &#x2192; nevergrad.optimization.base.ConfiguredOptimizer<a class="headerlink" href="#nevergrad.optimizers.base.ConfiguredOptimizer.set_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Set a new representation for the instance</p>
</dd></dl>

</dd></dl>

<p>Here is a list of the available configurable optimizers:</p>
<span class="target" id="module-nevergrad.families"></span><p>Parametrizable families of optimizers.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>This module and its available classes are experimental and may change quickly in the near future.</p>
</div>
<dl class="py class">
<dt id="nevergrad.families.Chaining">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.families.</code><code class="sig-name descname">Chaining</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">optimizers</span><span class="p">:</span> <span class="n">Sequence<span class="p">[</span>Union<span class="p">[</span>nevergrad.optimization.base.ConfiguredOptimizer<span class="p">, </span>Type<span class="p">[</span>nevergrad.optimization.base.Optimizer<span class="p">]</span><span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budgets</span><span class="p">:</span> <span class="n">Sequence<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span>int<span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.families.Chaining" title="Permalink to this definition">¶</a></dt>
<dd><p>A chaining consists in running algorithm 1 during T1, then algorithm 2 during T2, then algorithm 3 during T3, etc.
Each algorithm is fed with what happened before it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizers</strong> (<em>list of Optimizer classes</em>) – the sequence of optimizers to use</p></li>
<li><p><strong>budgets</strong> (<em>list of int</em>) – the corresponding budgets for each optimizer but the last one</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.families.DifferentialEvolution">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.families.</code><code class="sig-name descname">DifferentialEvolution</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">initialization</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'gaussian'</span></em>, <em class="sig-param"><span class="n">scale</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">recommendation</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'optimistic'</span></em>, <em class="sig-param"><span class="n">crossover</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">0.5</span></em>, <em class="sig-param"><span class="n">F1</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.8</span></em>, <em class="sig-param"><span class="n">F2</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.8</span></em>, <em class="sig-param"><span class="n">popsize</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>str<span class="p">, </span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">'standard'</span></em>, <em class="sig-param"><span class="n">propagate_heritage</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">multiobjective_adaptation</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.families.DifferentialEvolution" title="Permalink to this definition">¶</a></dt>
<dd><p>Differential evolution is typically used for continuous optimization.
It uses differences between points in the population for doing mutations in fruitful directions;
it is therefore a kind of covariance adaptation without any explicit covariance,
making it super fast in high dimension. This class implements several variants of differential
evolution, some of them adapted to genetic mutations as in
<a class="reference external" href="https://en.wikipedia.org/wiki/Crossover_(genetic_algorithm)#Two-point_and_k-point_crossover">Holland’s work</a>),
(this combination is termed <code class="code docutils literal notranslate"><span class="pre">TwoPointsDE</span></code> in Nevergrad, corresponding to <code class="code docutils literal notranslate"><span class="pre">crossover=&quot;twopoints&quot;</span></code>),
or to the noisy setting (coined <code class="code docutils literal notranslate"><span class="pre">NoisyDE</span></code>, corresponding to <code class="code docutils literal notranslate"><span class="pre">recommendation=&quot;noisy&quot;</span></code>).
In that last case, the optimizer returns the mean of the individuals with fitness better than median,
which might be stupid sometimes though.</p>
<p>Default settings are CR =.5, F1=.8, F2=.8, curr-to-best, pop size is 30
Initial population: pure random.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>initialization</strong> (<em>&quot;LHS&quot;</em><em>, </em><em>&quot;QR&quot;</em><em> or </em><em>&quot;gaussian&quot;</em>) – algorithm/distribution used for the initialization phase</p></li>
<li><p><strong>scale</strong> (<em>float</em><em> or </em><em>str</em>) – scale of random component of the updates</p></li>
<li><p><strong>recommendation</strong> (<em>&quot;pessimistic&quot;</em><em>, </em><em>&quot;optimistic&quot;</em><em>, </em><em>&quot;mean&quot;</em><em> or </em><em>&quot;noisy&quot;</em>) – choice of the criterion for the best point to recommend</p></li>
<li><p><strong>crossover</strong> (<em>float</em><em> or </em><em>str</em>) – crossover rate value, or strategy among:
- “dimension”: crossover rate of  1 / dimension,
- “random”: different random (uniform) crossover rate at each iteration
- “onepoint”: one point crossover
- “twopoints”: two points crossover
- “parametrization”: use the parametrization recombine method</p></li>
<li><p><strong>F1</strong> (<em>float</em>) – differential weight #1</p></li>
<li><p><strong>F2</strong> (<em>float</em>) – differential weight #2</p></li>
<li><p><strong>popsize</strong> (<em>int</em><em>, </em><em>&quot;standard&quot;</em><em>, </em><em>&quot;dimension&quot;</em><em>, </em><em>&quot;large&quot;</em>) – size of the population to use. “standard” is max(num_workers, 30), “dimension” max(num_workers, 30, dimension +1)
and “large” max(num_workers, 30, 7 * dimension).</p></li>
<li><p><strong>multiobjective_adaptation</strong> (<em>bool</em>) – Automatically adapts to handle multiobjective case.  This is a very basic <strong>experimental</strong> version,
activated by default because the non-multiobjective implementation is performing very badly.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.families.EMNA">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.families.</code><code class="sig-name descname">EMNA</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">isotropic</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">naive</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">population_size_adaptation</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">initial_popsize</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.families.EMNA" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimation of Multivariate Normal Algorithm
This algorithm is quite efficient in a parallel context, i.e. when
the population size is large.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>isotropic</strong> (<em>bool</em>) – isotropic version on EMNA if True, i.e. we have an
identity matrix for the Gaussian, else  we here consider the separable
version, meaning we have a diagonal matrix for the Gaussian (anisotropic)</p></li>
<li><p><strong>naive</strong> (<em>bool</em>) – set to False for noisy problem, so that the best points will be an
average of the final population.</p></li>
<li><p><strong>population_size_adaptation</strong> (<em>bool</em>) – population size automatically adapts to the landscape</p></li>
<li><p><strong>initial_popsize</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – initial (and minimal) population size (default: 4 x dimension)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.families.EvolutionStrategy">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.families.</code><code class="sig-name descname">EvolutionStrategy</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">recombination_ratio</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0</span></em>, <em class="sig-param"><span class="n">popsize</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">40</span></em>, <em class="sig-param"><span class="n">offsprings</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">only_offsprings</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">ranker</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'nsga2'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.families.EvolutionStrategy" title="Permalink to this definition">¶</a></dt>
<dd><p>Experimental evolution-strategy-like algorithm
The API is going to evolve</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>recombination_ratio</strong> (<em>float</em>) – probability of using a recombination (after the mutation) for generating new offsprings</p></li>
<li><p><strong>popsize</strong> (<em>int</em>) – population size of the parents (lambda)</p></li>
<li><p><strong>offsprings</strong> (<em>int</em>) – number of generated offsprings (mu)</p></li>
<li><p><strong>only_offsprings</strong> (<em>bool</em>) – use only offsprings for the new generation if True (True: lambda,mu, False: lambda+mu)</p></li>
<li><p><strong>ranker</strong> (<em>str</em>) – ranker for the multiobjective case (defaults to NSGA2)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.families.ParametrizedBO">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.families.</code><code class="sig-name descname">ParametrizedBO</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">initialization</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">init_budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">middle_point</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">utility_kind</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'ucb'</span></em>, <em class="sig-param"><span class="n">utility_kappa</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">2.576</span></em>, <em class="sig-param"><span class="n">utility_xi</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">gp_parameters</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.families.ParametrizedBO" title="Permalink to this definition">¶</a></dt>
<dd><p>Bayesian optimization.
Hyperparameter tuning method, based on statistical modeling of the objective function.
This class is a wrapper over the <a class="reference external" href="https://github.com/fmfn/BayesianOptimization">bayes_opt</a> package.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>initialization</strong> (<em>str</em>) – Initialization algorithms (None, “Hammersley”, “random” or “LHS”)</p></li>
<li><p><strong>init_budget</strong> (<em>int</em><em> or </em><em>None</em>) – Number of initialization algorithm steps</p></li>
<li><p><strong>middle_point</strong> (<em>bool</em>) – whether to sample the 0 point first</p></li>
<li><p><strong>utility_kind</strong> (<em>str</em>) – Type of utility function to use among “ucb”, “ei” and “poi”</p></li>
<li><p><strong>utility_kappa</strong> (<em>float</em>) – Kappa parameter for the utility function</p></li>
<li><p><strong>utility_xi</strong> (<em>float</em>) – Xi parameter for the utility function</p></li>
<li><p><strong>gp_parameters</strong> (<em>dict</em>) – dictionnary of parameters for the gaussian process</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.families.ParametrizedCMA">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.families.</code><code class="sig-name descname">ParametrizedCMA</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">scale</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">elitist</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">popsize</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">diagonal</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">fcmaes</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">random_init</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.families.ParametrizedCMA" title="Permalink to this definition">¶</a></dt>
<dd><p>CMA-ES optimizer,
This evolution strategy uses a Gaussian sampling, iteratively modified
for searching in the best directions.
This optimizer wraps an external implementation: <a class="reference external" href="https://github.com/CMA-ES/pycma">https://github.com/CMA-ES/pycma</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scale</strong> (<em>float</em>) – scale of the search</p></li>
<li><p><strong>elitist</strong> (<em>bool</em>) – whether we switch to elitist mode, i.e. mode + instead of comma,
i.e. mode in which we always keep the best point in the population.</p></li>
<li><p><strong>popsize</strong> (<em>Optional</em><em>[</em><em>int</em><em>] </em><em>= None</em>) – population size, should be n * self.num_workers for int n &gt;= 1.
default is max(self.num_workers, 4 + int(3 * np.log(self.dimension)))</p></li>
<li><p><strong>diagonal</strong> (<em>bool</em>) – use the diagonal version of CMA (advised in big dimension)</p></li>
<li><p><strong>fcmaes</strong> (<em>bool = False</em>) – use fast implementation, doesn’t support diagonal=True.
produces equivalent results, preferable for high dimensions or
if objective function evaluation is fast.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.families.ParametrizedOnePlusOne">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.families.</code><code class="sig-name descname">ParametrizedOnePlusOne</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">noise_handling</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span>Tuple<span class="p">[</span>str<span class="p">, </span>float<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">mutation</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'gaussian'</span></em>, <em class="sig-param"><span class="n">crossover</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.families.ParametrizedOnePlusOne" title="Permalink to this definition">¶</a></dt>
<dd><p>Simple but sometimes powerfull class of optimization algorithm.
This use asynchronous updates, so that (1+1) can actually be parallel and even
performs quite well in such a context - this is naturally close to (1+lambda).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>noise_handling</strong> (<em>str</em><em> or </em><a class="reference internal" href="parametrization_ref.html#nevergrad.p.Tuple" title="nevergrad.p.Tuple"><em>Tuple</em></a><em>[</em><em>str</em><em>, </em><em>float</em><em>]</em>) – <p>Method for handling the noise. The name can be:</p>
<ul>
<li><p><cite>”random”</cite>: a random point is reevaluated regularly, this uses the one-fifth adaptation rule,
going back to Schumer and Steiglitz (1968). It was independently rediscovered by Devroye (1972) and Rechenberg (1973).</p></li>
<li><p><cite>”optimistic”</cite>: the best optimistic point is reevaluated regularly, optimism in front of uncertainty</p></li>
<li><p>a coefficient can to tune the regularity of these reevaluations (default .05)</p></li>
</ul>
</p></li>
<li><p><strong>mutation</strong> (<em>str</em>) – <p>One of the available mutations from:</p>
<ul>
<li><p><cite>”gaussian”</cite>: standard mutation by adding a Gaussian random variable (with progressive
widening) to the best pessimistic point</p></li>
<li><p><cite>”cauchy”</cite>: same as Gaussian but with a Cauchy distribution.</p></li>
<li><dl class="simple">
<dt><cite>”discrete”</cite>: when a variable is mutated (which happens with probability 1/d in dimension d), it’s just</dt><dd><p>randomly drawn. This means that on average, only one variable is mutated.</p>
</dd>
</dl>
</li>
<li><p><cite>”discreteBSO”</cite>: as in brainstorm optimization, we slowly decrease the mutation rate from 1 to 1/d.</p></li>
<li><p><cite>”fastga”</cite>: FastGA mutations from the current best</p></li>
<li><p><cite>”doublefastga”</cite>: double-FastGA mutations from the current best (Doerr et al, Fast Genetic Algorithms, 2017)</p></li>
<li><p><cite>”portfolio”</cite>: Random number of mutated bits (called niform mixing in
Dang &amp; Lehre “Self-adaptation of Mutation Rates in Non-elitist Population”, 2016)</p></li>
<li><p><cite>”lengler”</cite>: specific mutation rate chosen as a function of the dimension and iteration index.</p></li>
</ul>
</p></li>
<li><p><strong>crossover</strong> (<em>bool</em>) – whether to add a genetic crossover step every other iteration.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>After many papers advocared the mutation rate 1/d in the discrete (1+1) for the discrete case,
<a class="reference external" href="https://arxiv.org/abs/1606.05551">it was proposed</a> to use of a randomly
drawn mutation rate. <a class="reference external" href="https://arxiv.org/abs/1703.03334">Fast genetic algorithms</a> are based on a similar idea
These two simple methods perform quite well on a wide range of problems.</p>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.families.ParametrizedTBPSA">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.families.</code><code class="sig-name descname">ParametrizedTBPSA</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">naive</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">initial_popsize</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.families.ParametrizedTBPSA" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://homepages.fhv.at/hgb/New-Papers/PPSN16_HB16.pdf">Test-based population-size adaptation</a>
This method, based on adapting the population size, performs the best in
many noisy optimization problems, even in large dimension</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>naive</strong> (<em>bool</em>) – set to False for noisy problem, so that the best points will be an
average of the final population.</p></li>
<li><p><strong>initial_popsize</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – initial (and minimal) population size (default: 4 x dimension)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Derived from:
Hellwig, Michael &amp; Beyer, Hans-Georg. (2016).
Evolution under Strong Noise: A Self-Adaptive Evolution Strategy
Reaches the Lower Performance Bound – the pcCMSA-ES.
<a class="reference external" href="https://homepages.fhv.at/hgb/New-Papers/PPSN16_HB16.pdf">https://homepages.fhv.at/hgb/New-Papers/PPSN16_HB16.pdf</a></p>
</div>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.families.RandomSearchMaker">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.families.</code><code class="sig-name descname">RandomSearchMaker</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">middle_point</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">stupid</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">opposition_mode</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">cauchy</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">scale</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>float<span class="p">, </span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">recommendation_rule</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'pessimistic'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.families.RandomSearchMaker" title="Permalink to this definition">¶</a></dt>
<dd><p>Provides random suggestions.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>stupid</strong> (<em>bool</em>) – Provides a random recommendation instead of the best point so far (for baseline)</p></li>
<li><p><strong>middle_point</strong> (<em>bool</em>) – enforces that the first suggested point (ask) is zero.</p></li>
<li><p><strong>opposition_mode</strong> (<em>str</em><em> or </em><em>None</em>) – <dl class="simple">
<dt>symmetrizes exploration wrt the center: (e.g. <a class="reference external" href="https://ieeexplore.ieee.org/document/4424748">https://ieeexplore.ieee.org/document/4424748</a>)</dt><dd><ul>
<li><p>full symmetry if “opposite”</p></li>
<li><p>random * symmetric if “quasi”</p></li>
</ul>
</dd>
</dl>
</p></li>
<li><p><strong>cauchy</strong> (<em>bool</em>) – use a Cauchy distribution instead of Gaussian distribution</p></li>
<li><p><strong>scale</strong> (<em>float</em><em> or </em><em>&quot;random&quot;</em>) – <dl class="simple">
<dt>scalar for multiplying the suggested point values, or string:</dt><dd><ul>
<li><p>”random”: uses a randomized pattern for the scale.</p></li>
<li><p>”auto”: scales in function of dimension and budget (version 1: sigma = (1+log(budget)) / (4log(dimension)) )</p></li>
<li><p>”autotune”: scales in function of dimension and budget (version 2: sigma = sqrt(log(budget) / dimension) )</p></li>
</ul>
</dd>
</dl>
</p></li>
<li><p><strong>recommendation_rule</strong> (<em>str</em>) – “average_of_best” or “pessimistic” or “average_of_exp_best”; “pessimistic” is
the default and implies selecting the pessimistic best.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.families.SamplingSearch">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.families.</code><code class="sig-name descname">SamplingSearch</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">sampler</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'Halton'</span></em>, <em class="sig-param"><span class="n">scrambled</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">middle_point</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">opposition_mode</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">cauchy</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">autorescale</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>bool<span class="p">, </span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">scale</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">rescaled</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">recommendation_rule</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'pessimistic'</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.families.SamplingSearch" title="Permalink to this definition">¶</a></dt>
<dd><p>This is a one-shot optimization method, hopefully better than random search
by ensuring more uniformity.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>sampler</strong> (<em>str</em>) – Choice of the sampler among “Halton”, “Hammersley” and “LHS”.</p></li>
<li><p><strong>scrambled</strong> (<em>bool</em>) – Adds scrambling to the search; much better in high dimension and rarely worse
than the original search.</p></li>
<li><p><strong>middle_point</strong> (<em>bool</em>) – enforces that the first suggested point (ask) is zero.</p></li>
<li><p><strong>cauchy</strong> (<em>bool</em>) – use Cauchy inverse distribution instead of Gaussian when fitting points to real space
(instead of box).</p></li>
<li><p><strong>scale</strong> (<em>float</em><em> or </em><em>&quot;random&quot;</em>) – scalar for multiplying the suggested point values.</p></li>
<li><p><strong>rescaled</strong> (<em>bool</em><em> or </em><em>str</em>) – rescales the sampling pattern to reach the boundaries and/or applies automatic rescaling.</p></li>
<li><p><strong>recommendation_rule</strong> (<em>str</em>) – “average_of_best” or “pessimistic”; “pessimistic” is the default and implies selecting the pessimistic best.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Notes</p>
<ul class="simple">
<li><p>Halton is a low quality sampling method when the dimension is high; it is usually better
to use Halton with scrambling.</p></li>
<li><p>When the budget is known in advance, it is also better to replace Halton by Hammersley.
Basically the key difference with Halton is adding one coordinate evenly spaced
(the discrepancy is better).
budget, low discrepancy sequences (e.g. scrambled Hammersley) have a better discrepancy.</p></li>
<li><p>Reference: Halton 1964: Algorithm 247: Radical-inverse quasi-random point sequence, ACM, p. 701.
adds scrambling to the Halton search; much better in high dimension and rarely worse
than the original Halton search.</p></li>
<li><p>About Latin Hypercube Sampling (LHS):
Though partially incremental versions exist, this implementation needs the budget in advance.
This can be great in terms of discrepancy when the budget is not very high.</p></li>
</ul>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.families.ScipyOptimizer">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.families.</code><code class="sig-name descname">ScipyOptimizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">method</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'Nelder-Mead'</span></em>, <em class="sig-param"><span class="n">random_restart</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.families.ScipyOptimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Wrapper over Scipy optimizer implementations, in standard ask and tell format.
This is actually an import from scipy-optimize, including Sequential Quadratic Programming,</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>method</strong> (<em>str</em>) – <p>Name of the method to use among:</p>
<ul>
<li><p>Nelder-Mead</p></li>
<li><p>COBYLA</p></li>
<li><p>SQP (or SLSQP): very powerful e.g. in continuous noisy optimization. It is based on
approximating the objective function by quadratic models.</p></li>
<li><p>Powell</p></li>
</ul>
</p></li>
<li><p><strong>random_restart</strong> (<em>bool</em>) – whether to restart at a random point if the optimizer converged but the budget is not entirely
spent yet (otherwise, restarts from best point)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>These optimizers do not support asking several candidates in a row</p>
</div>
</dd></dl>

</div>
<div class="section" id="optimizers">
<h2>Optimizers<a class="headerlink" href="#optimizers" title="Permalink to this headline">¶</a></h2>
<p>Here are all the other optimizers available in <code class="code docutils literal notranslate"><span class="pre">nevergrad</span></code>:</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>Only non-family-based optimizers are listed in the documentation,
you can get a full list of available optimizers with <code class="code docutils literal notranslate"><span class="pre">sorted(nevergrad.optimizers.registry.keys())</span></code></p>
</div>
<span class="target" id="module-nevergrad.optimization.optimizerlib"></span><dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.ASCMADEthird">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">ASCMADEthird</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.ASCMADEthird" title="Permalink to this definition">¶</a></dt>
<dd><p>Algorithm selection, with CMA and Lhs-DE. Active selection at 1/3.</p>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.CM">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">CM</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.CM" title="Permalink to this definition">¶</a></dt>
<dd><p>Competence map, simplest.</p>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.CMandAS2">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">CMandAS2</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.CMandAS2" title="Permalink to this definition">¶</a></dt>
<dd><p>Competence map, with algorithm selection in one of the cases (3 CMAs).</p>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.CMandAS3">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">CMandAS3</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.CMandAS3" title="Permalink to this definition">¶</a></dt>
<dd><p>Competence map, with algorithm selection in one of the cases (3 CMAs).</p>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.Chaining">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">Chaining</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">optimizers</span><span class="p">:</span> <span class="n">Sequence<span class="p">[</span>Union<span class="p">[</span>nevergrad.optimization.base.ConfiguredOptimizer<span class="p">, </span>Type<span class="p">[</span>nevergrad.optimization.base.Optimizer<span class="p">]</span><span class="p">]</span><span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budgets</span><span class="p">:</span> <span class="n">Sequence<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span>int<span class="p">]</span><span class="p">]</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.Chaining" title="Permalink to this definition">¶</a></dt>
<dd><p>A chaining consists in running algorithm 1 during T1, then algorithm 2 during T2, then algorithm 3 during T3, etc.
Each algorithm is fed with what happened before it.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>optimizers</strong> (<em>list of Optimizer classes</em>) – the sequence of optimizers to use</p></li>
<li><p><strong>budgets</strong> (<em>list of int</em>) – the corresponding budgets for each optimizer but the last one</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.ConfSplitOptimizer">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">ConfSplitOptimizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">num_optims</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_vars</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">multivariate_optimizer</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>ConfiguredOptimizer<span class="p">, </span>Type<span class="p">[</span>Optimizer<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">CMA</span></em>, <em class="sig-param"><span class="n">monovariate_optimizer</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>ConfiguredOptimizer<span class="p">, </span>Type<span class="p">[</span>Optimizer<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">RandomSearch</span></em>, <em class="sig-param"><span class="n">progressive</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">non_deterministic_descriptor</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.ConfSplitOptimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>“Combines optimizers, each of them working on their own variables.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_optims</strong> (<em>int</em>) – number of optimizers</p></li>
<li><p><strong>num_vars</strong> (<em>optional list of int</em>) – number of variable per optimizer.</p></li>
<li><p><strong>progressive</strong> (<em>optional bool</em>) – whether we progressively add optimizers.</p></li>
<li><p><strong>non_deterministic_descriptor</strong> (<em>bool</em>) – subparts parametrization descriptor is set to noisy function.
This can have an impact for optimizer selection for competence maps.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.ConfiguredPSO">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">ConfiguredPSO</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">transform</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'identity'</span></em>, <em class="sig-param"><span class="n">wide</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">popsize</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.ConfiguredPSO" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://en.wikipedia.org/wiki/Particle_swarm_optimization">Particle Swarm Optimization</a>
is based on a set of particles with their inertia.
Wikipedia provides a beautiful illustration ;) (see link)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>transform</strong> (<em>str</em>) – name of the transform to use to map from PSO optimization space to R-space.</p></li>
<li><p><strong>wide</strong> (<em>bool</em>) – if True: legacy initialization in [-1,1] box mapped to R</p></li>
<li><p><strong>popsize</strong> (<em>int</em>) – population size of the particle swarm. Defaults to max(40, num_workers)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>Using non-default “transform” and “wide” parameters can lead to extreme values</p></li>
<li><p>Implementation partially following SPSO2011. However, no randomization of the population order.</p></li>
<li><p>Reference:
M. Zambrano-Bigiarini, M. Clerc and R. Rojas,
Standard Particle Swarm Optimisation 2011 at CEC-2013: A baseline for future PSO improvements,
2013 IEEE Congress on Evolutionary Computation, Cancun, 2013, pp. 2337-2344.
<a class="reference external" href="https://ieeexplore.ieee.org/document/6557848">https://ieeexplore.ieee.org/document/6557848</a></p></li>
</ul>
</div>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.EDA">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">EDA</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.EDA" title="Permalink to this definition">¶</a></dt>
<dd><p>Test-based population-size adaptation.</p>
<p>Population-size equal to lambda = 4 x dimension.
Test by comparing the first fifth and the last fifth of the 5lambda evaluations.</p>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>This optimizer is probably wrong.</p>
</div>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.EMNA">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">EMNA</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">isotropic</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">naive</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">population_size_adaptation</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">initial_popsize</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.EMNA" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimation of Multivariate Normal Algorithm
This algorithm is quite efficient in a parallel context, i.e. when
the population size is large.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>isotropic</strong> (<em>bool</em>) – isotropic version on EMNA if True, i.e. we have an
identity matrix for the Gaussian, else  we here consider the separable
version, meaning we have a diagonal matrix for the Gaussian (anisotropic)</p></li>
<li><p><strong>naive</strong> (<em>bool</em>) – set to False for noisy problem, so that the best points will be an
average of the final population.</p></li>
<li><p><strong>population_size_adaptation</strong> (<em>bool</em>) – population size automatically adapts to the landscape</p></li>
<li><p><strong>initial_popsize</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – initial (and minimal) population size (default: 4 x dimension)</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py exception">
<dt id="nevergrad.optimization.optimizerlib.InfiniteMetaModelOptimum">
<em class="property">exception </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">InfiniteMetaModelOptimum</code><a class="headerlink" href="#nevergrad.optimization.optimizerlib.InfiniteMetaModelOptimum" title="Permalink to this definition">¶</a></dt>
<dd><p>Sometimes the optimum of the metamodel is at infinity.</p>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.MEDA">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">MEDA</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.MEDA" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.MPCEDA">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">MPCEDA</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.MPCEDA" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.MetaModel">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">MetaModel</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">multivariate_optimizer</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>ConfiguredOptimizer<span class="p">, </span>Type<span class="p">[</span>Optimizer<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.MetaModel" title="Permalink to this definition">¶</a></dt>
<dd><p>Adding a metamodel into CMA.</p>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.MultiCMA">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">MultiCMA</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.MultiCMA" title="Permalink to this definition">¶</a></dt>
<dd><p>Combining 3 CMAs. Exactly identical. Active selection at 1/10 of the budget.</p>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.MultiDiscrete">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">MultiDiscrete</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.MultiDiscrete" title="Permalink to this definition">¶</a></dt>
<dd><p>Combining 3 Discrete(1+1). Exactly identical. Active selection at 1/10 of the budget.</p>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.MultiScaleCMA">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">MultiScaleCMA</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.MultiScaleCMA" title="Permalink to this definition">¶</a></dt>
<dd><p>Combining 3 CMAs with different init scale. Active selection at 1/3 of the budget.</p>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.NGO">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">NGO</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.NGO" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.NGOpt">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">NGOpt</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.NGOpt" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.NGOpt10">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">NGOpt10</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.NGOpt10" title="Permalink to this definition">¶</a></dt>
<dd><dl class="py method">
<dt id="nevergrad.optimization.optimizerlib.NGOpt10.recommend">
<code class="sig-name descname">recommend</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; nevergrad.parametrization.core.Parameter<a class="headerlink" href="#nevergrad.optimization.optimizerlib.NGOpt10.recommend" title="Permalink to this definition">¶</a></dt>
<dd><p>Provides the best candidate to use as a minimum, given the budget that was used.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The candidate with minimal loss. <code class="code docutils literal notranslate"><span class="pre">p.Parameters</span></code> have field <code class="code docutils literal notranslate"><span class="pre">args</span></code> and <code class="code docutils literal notranslate"><span class="pre">kwargs</span></code> which can be directly used
on the function (<code class="code docutils literal notranslate"><span class="pre">objective_function(*candidate.args,</span> <span class="pre">**candidate.kwargs)</span></code>).</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="parametrization_ref.html#nevergrad.p.Parameter" title="nevergrad.p.Parameter">p.Parameter</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.NGOpt4">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">NGOpt4</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.NGOpt4" title="Permalink to this definition">¶</a></dt>
<dd><p>Nevergrad optimizer by competence map. You might modify this one for designing youe own competence map.</p>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.NGOpt8">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">NGOpt8</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.NGOpt8" title="Permalink to this definition">¶</a></dt>
<dd><p>Nevergrad optimizer by competence map. You might modify this one for designing youe own competence map.</p>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.NGOptBase">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">NGOptBase</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.NGOptBase" title="Permalink to this definition">¶</a></dt>
<dd><p>Nevergrad optimizer by competence map.</p>
<dl class="py method">
<dt id="nevergrad.optimization.optimizerlib.NGOptBase.optim">
<em class="property">property </em><code class="sig-name descname">optim</code><a class="headerlink" href="#nevergrad.optimization.optimizerlib.NGOptBase.optim" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py method">
<dt id="nevergrad.optimization.optimizerlib.NGOptBase.recommend">
<code class="sig-name descname">recommend</code><span class="sig-paren">(</span><span class="sig-paren">)</span> &#x2192; nevergrad.parametrization.core.Parameter<a class="headerlink" href="#nevergrad.optimization.optimizerlib.NGOptBase.recommend" title="Permalink to this definition">¶</a></dt>
<dd><p>Provides the best candidate to use as a minimum, given the budget that was used.</p>
<dl class="field-list simple">
<dt class="field-odd">Returns</dt>
<dd class="field-odd"><p>The candidate with minimal loss. <code class="code docutils literal notranslate"><span class="pre">p.Parameters</span></code> have field <code class="code docutils literal notranslate"><span class="pre">args</span></code> and <code class="code docutils literal notranslate"><span class="pre">kwargs</span></code> which can be directly used
on the function (<code class="code docutils literal notranslate"><span class="pre">objective_function(*candidate.args,</span> <span class="pre">**candidate.kwargs)</span></code>).</p>
</dd>
<dt class="field-even">Return type</dt>
<dd class="field-even"><p><a class="reference internal" href="parametrization_ref.html#nevergrad.p.Parameter" title="nevergrad.p.Parameter">p.Parameter</a></p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.NoisyBandit">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">NoisyBandit</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.NoisyBandit" title="Permalink to this definition">¶</a></dt>
<dd><p>UCB.
This is upper confidence bound (adapted to minimization),
with very poor parametrization; in particular, the logarithmic term is set to zero.
Infinite arms: we add one arm when <cite>20 * #ask &gt;= #arms ** 3</cite>.</p>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.PCEDA">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">PCEDA</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.PCEDA" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.PSO">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">PSO</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">transform</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'arctan'</span></em>, <em class="sig-param"><span class="n">wide</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">popsize</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.PSO" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.ParaPortfolio">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">ParaPortfolio</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.ParaPortfolio" title="Permalink to this definition">¶</a></dt>
<dd><p>Passive portfolio of CMA, 2-pt DE, PSO, SQP and Scr-Hammersley.</p>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.ParametrizedBO">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">ParametrizedBO</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">initialization</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>str<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">init_budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">middle_point</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">utility_kind</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'ucb'</span></em>, <em class="sig-param"><span class="n">utility_kappa</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">2.576</span></em>, <em class="sig-param"><span class="n">utility_xi</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">0.0</span></em>, <em class="sig-param"><span class="n">gp_parameters</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Dict<span class="p">[</span>str<span class="p">, </span>Any<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.ParametrizedBO" title="Permalink to this definition">¶</a></dt>
<dd><p>Bayesian optimization.
Hyperparameter tuning method, based on statistical modeling of the objective function.
This class is a wrapper over the <a class="reference external" href="https://github.com/fmfn/BayesianOptimization">bayes_opt</a> package.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>initialization</strong> (<em>str</em>) – Initialization algorithms (None, “Hammersley”, “random” or “LHS”)</p></li>
<li><p><strong>init_budget</strong> (<em>int</em><em> or </em><em>None</em>) – Number of initialization algorithm steps</p></li>
<li><p><strong>middle_point</strong> (<em>bool</em>) – whether to sample the 0 point first</p></li>
<li><p><strong>utility_kind</strong> (<em>str</em>) – Type of utility function to use among “ucb”, “ei” and “poi”</p></li>
<li><p><strong>utility_kappa</strong> (<em>float</em>) – Kappa parameter for the utility function</p></li>
<li><p><strong>utility_xi</strong> (<em>float</em>) – Xi parameter for the utility function</p></li>
<li><p><strong>gp_parameters</strong> (<em>dict</em>) – dictionnary of parameters for the gaussian process</p></li>
</ul>
</dd>
</dl>
<dl class="py attribute">
<dt id="nevergrad.optimization.optimizerlib.ParametrizedBO.no_parallelization">
<code class="sig-name descname">no_parallelization</code><em class="property"> = True</em><a class="headerlink" href="#nevergrad.optimization.optimizerlib.ParametrizedBO.no_parallelization" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.ParametrizedCMA">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">ParametrizedCMA</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">scale</span><span class="p">:</span> <span class="n">float</span> <span class="o">=</span> <span class="default_value">1.0</span></em>, <em class="sig-param"><span class="n">elitist</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">popsize</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">diagonal</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">fcmaes</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">random_init</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.ParametrizedCMA" title="Permalink to this definition">¶</a></dt>
<dd><p>CMA-ES optimizer,
This evolution strategy uses a Gaussian sampling, iteratively modified
for searching in the best directions.
This optimizer wraps an external implementation: <a class="reference external" href="https://github.com/CMA-ES/pycma">https://github.com/CMA-ES/pycma</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>scale</strong> (<em>float</em>) – scale of the search</p></li>
<li><p><strong>elitist</strong> (<em>bool</em>) – whether we switch to elitist mode, i.e. mode + instead of comma,
i.e. mode in which we always keep the best point in the population.</p></li>
<li><p><strong>popsize</strong> (<em>Optional</em><em>[</em><em>int</em><em>] </em><em>= None</em>) – population size, should be n * self.num_workers for int n &gt;= 1.
default is max(self.num_workers, 4 + int(3 * np.log(self.dimension)))</p></li>
<li><p><strong>diagonal</strong> (<em>bool</em>) – use the diagonal version of CMA (advised in big dimension)</p></li>
<li><p><strong>fcmaes</strong> (<em>bool = False</em>) – use fast implementation, doesn’t support diagonal=True.
produces equivalent results, preferable for high dimensions or
if objective function evaluation is fast.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.ParametrizedOnePlusOne">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">ParametrizedOnePlusOne</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">noise_handling</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>Union<span class="p">[</span>str<span class="p">, </span>Tuple<span class="p">[</span>str<span class="p">, </span>float<span class="p">]</span><span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">mutation</span><span class="p">:</span> <span class="n">str</span> <span class="o">=</span> <span class="default_value">'gaussian'</span></em>, <em class="sig-param"><span class="n">crossover</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.ParametrizedOnePlusOne" title="Permalink to this definition">¶</a></dt>
<dd><p>Simple but sometimes powerfull class of optimization algorithm.
This use asynchronous updates, so that (1+1) can actually be parallel and even
performs quite well in such a context - this is naturally close to (1+lambda).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>noise_handling</strong> (<em>str</em><em> or </em><a class="reference internal" href="parametrization_ref.html#nevergrad.p.Tuple" title="nevergrad.p.Tuple"><em>Tuple</em></a><em>[</em><em>str</em><em>, </em><em>float</em><em>]</em>) – <p>Method for handling the noise. The name can be:</p>
<ul>
<li><p><cite>”random”</cite>: a random point is reevaluated regularly, this uses the one-fifth adaptation rule,
going back to Schumer and Steiglitz (1968). It was independently rediscovered by Devroye (1972) and Rechenberg (1973).</p></li>
<li><p><cite>”optimistic”</cite>: the best optimistic point is reevaluated regularly, optimism in front of uncertainty</p></li>
<li><p>a coefficient can to tune the regularity of these reevaluations (default .05)</p></li>
</ul>
</p></li>
<li><p><strong>mutation</strong> (<em>str</em>) – <p>One of the available mutations from:</p>
<ul>
<li><p><cite>”gaussian”</cite>: standard mutation by adding a Gaussian random variable (with progressive
widening) to the best pessimistic point</p></li>
<li><p><cite>”cauchy”</cite>: same as Gaussian but with a Cauchy distribution.</p></li>
<li><dl class="simple">
<dt><cite>”discrete”</cite>: when a variable is mutated (which happens with probability 1/d in dimension d), it’s just</dt><dd><p>randomly drawn. This means that on average, only one variable is mutated.</p>
</dd>
</dl>
</li>
<li><p><cite>”discreteBSO”</cite>: as in brainstorm optimization, we slowly decrease the mutation rate from 1 to 1/d.</p></li>
<li><p><cite>”fastga”</cite>: FastGA mutations from the current best</p></li>
<li><p><cite>”doublefastga”</cite>: double-FastGA mutations from the current best (Doerr et al, Fast Genetic Algorithms, 2017)</p></li>
<li><p><cite>”portfolio”</cite>: Random number of mutated bits (called niform mixing in
Dang &amp; Lehre “Self-adaptation of Mutation Rates in Non-elitist Population”, 2016)</p></li>
<li><p><cite>”lengler”</cite>: specific mutation rate chosen as a function of the dimension and iteration index.</p></li>
</ul>
</p></li>
<li><p><strong>crossover</strong> (<em>bool</em>) – whether to add a genetic crossover step every other iteration.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>After many papers advocared the mutation rate 1/d in the discrete (1+1) for the discrete case,
<a class="reference external" href="https://arxiv.org/abs/1606.05551">it was proposed</a> to use of a randomly
drawn mutation rate. <a class="reference external" href="https://arxiv.org/abs/1703.03334">Fast genetic algorithms</a> are based on a similar idea
These two simple methods perform quite well on a wide range of problems.</p>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.ParametrizedTBPSA">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">ParametrizedTBPSA</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">naive</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em>, <em class="sig-param"><span class="n">initial_popsize</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.ParametrizedTBPSA" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://homepages.fhv.at/hgb/New-Papers/PPSN16_HB16.pdf">Test-based population-size adaptation</a>
This method, based on adapting the population size, performs the best in
many noisy optimization problems, even in large dimension</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>naive</strong> (<em>bool</em>) – set to False for noisy problem, so that the best points will be an
average of the final population.</p></li>
<li><p><strong>initial_popsize</strong> (<em>Optional</em><em>[</em><em>int</em><em>]</em>) – initial (and minimal) population size (default: 4 x dimension)</p></li>
</ul>
</dd>
</dl>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Derived from:
Hellwig, Michael &amp; Beyer, Hans-Georg. (2016).
Evolution under Strong Noise: A Self-Adaptive Evolution Strategy
Reaches the Lower Performance Bound – the pcCMSA-ES.
<a class="reference external" href="https://homepages.fhv.at/hgb/New-Papers/PPSN16_HB16.pdf">https://homepages.fhv.at/hgb/New-Papers/PPSN16_HB16.pdf</a></p>
</div>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.PolyCMA">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">PolyCMA</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.PolyCMA" title="Permalink to this definition">¶</a></dt>
<dd><p>Combining 20 CMAs. Exactly identical. Active selection at 1/3 of the budget.</p>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.Portfolio">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">Portfolio</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.Portfolio" title="Permalink to this definition">¶</a></dt>
<dd><p>Passive portfolio of CMA, 2-pt DE and Scr-Hammersley.</p>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.Rescaled">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">Rescaled</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span></em>, <em class="sig-param"><span class="n">base_optimizer</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>ConfiguredOptimizer<span class="p">, </span>Type<span class="p">[</span>Optimizer<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">CMA</span></em>, <em class="sig-param"><span class="n">scale</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>float<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.Rescaled" title="Permalink to this definition">¶</a></dt>
<dd><p>Configured optimizer for creating rescaled optimization algorithms.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>base_optimizer</strong> (<em>base.OptCls</em>) – optimization algorithm to be rescaled.</p></li>
<li><p><strong>scale</strong> (<em>how much do we rescale. E.g. 0.001 if we want to focus on the center</em>) – with std 0.001 (assuming the std of the domain is set to 1).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.SPSA">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">SPSA</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.SPSA" title="Permalink to this definition">¶</a></dt>
<dd><p>The First order SPSA algorithm as shown in [1,2,3], with implementation details
from [4,5].</p>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Simultaneous_perturbation_stochastic_approximation">https://en.wikipedia.org/wiki/Simultaneous_perturbation_stochastic_approximation</a></p></li>
<li><p><a class="reference external" href="https://www.chessprogramming.org/SPSA">https://www.chessprogramming.org/SPSA</a></p></li>
<li><p>Spall, James C. “Multivariate stochastic approximation using a simultaneous perturbation gradient approximation.”
IEEE transactions on automatic control 37.3 (1992): 332-341.</p></li>
<li><p>Section 7.5.2 in “Introduction to Stochastic Search and Optimization: Estimation, Simulation and Control” by James C. Spall.</p></li>
<li><p>Pushpendre Rastogi, Jingyi Zhu, James C. Spall CISS (2016).
Efficient implementation of Enhanced Adaptive Simultaneous Perturbation Algorithms.</p></li>
</ol>
<dl class="py attribute">
<dt id="nevergrad.optimization.optimizerlib.SPSA.no_parallelization">
<code class="sig-name descname">no_parallelization</code><em class="property"> = True</em><a class="headerlink" href="#nevergrad.optimization.optimizerlib.SPSA.no_parallelization" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.SQPCMA">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">SQPCMA</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.SQPCMA" title="Permalink to this definition">¶</a></dt>
<dd><p>Passive portfolio of CMA and many SQP.</p>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.Shiwa">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">Shiwa</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.Shiwa" title="Permalink to this definition">¶</a></dt>
<dd><p>Nevergrad optimizer by competence map. You might modify this one for designing youe own competence map.</p>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.SplitOptimizer">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">SplitOptimizer</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">num_optims</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_vars</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>List<span class="p">[</span>int<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">multivariate_optimizer</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>ConfiguredOptimizer<span class="p">, </span>Type<span class="p">[</span>Optimizer<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">CMA</span></em>, <em class="sig-param"><span class="n">monovariate_optimizer</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>ConfiguredOptimizer<span class="p">, </span>Type<span class="p">[</span>Optimizer<span class="p">]</span><span class="p">]</span></span> <span class="o">=</span> <span class="default_value">OnePlusOne</span></em>, <em class="sig-param"><span class="n">progressive</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">False</span></em>, <em class="sig-param"><span class="n">non_deterministic_descriptor</span><span class="p">:</span> <span class="n">bool</span> <span class="o">=</span> <span class="default_value">True</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.SplitOptimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Combines optimizers, each of them working on their own variables.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_optims</strong> (<em>int</em><em> or </em><em>None</em>) – number of optimizers</p></li>
<li><p><strong>num_vars</strong> (<em>int</em><em> or </em><em>None</em>) – number of variable per optimizer.</p></li>
<li><p><strong>progressive</strong> (<em>bool</em>) – True if we want to progressively add optimizers during the optimization run.
If progressive = True, the optimizer is forced at OptimisticNoisyOnePlusOne.</p></li>
</ul>
</dd>
</dl>
<p class="rubric">Example</p>
<p>for 5 optimizers, each of them working on 2 variables, one can use:</p>
<p>opt = SplitOptimizer(parametrization=10, num_workers=3, num_optims=5, num_vars=[2, 2, 2, 2, 2])
or equivalently:
opt = SplitOptimizer(parametrization=10, num_workers=3, num_vars=[2, 2, 2, 2, 2])
Given that all optimizers have the same number of variables, one can also run:
opt = SplitOptimizer(parametrization=10, num_workers=3, num_optims=5)</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>By default, it uses CMA for multivariate groups and RandomSearch for monovariate groups.</p>
</div>
<div class="admonition caution">
<p class="admonition-title">Caution</p>
<p>The variables refer to the deep representation used by optimizers.
For example, a categorical variable with 5 possible values becomes 5 continuous variables.</p>
</div>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.TripleCMA">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">TripleCMA</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.TripleCMA" title="Permalink to this definition">¶</a></dt>
<dd><p>Combining 3 CMAs. Exactly identical. Active selection at 1/3 of the budget.</p>
</dd></dl>

<dl class="py class">
<dt id="nevergrad.optimization.optimizerlib.cGA">
<em class="property">class </em><code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">cGA</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">parametrization</span><span class="p">:</span> <span class="n">Union<span class="p">[</span>int<span class="p">, </span>nevergrad.parametrization.core.Parameter<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">budget</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em>, <em class="sig-param"><span class="n">num_workers</span><span class="p">:</span> <span class="n">int</span> <span class="o">=</span> <span class="default_value">1</span></em>, <em class="sig-param"><span class="n">arity</span><span class="p">:</span> <span class="n">Optional<span class="p">[</span>int<span class="p">]</span></span> <span class="o">=</span> <span class="default_value">None</span></em><span class="sig-paren">)</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.cGA" title="Permalink to this definition">¶</a></dt>
<dd><p><a class="reference external" href="https://ieeexplore.ieee.org/document/797971">Compact Genetic Algorithm</a>.
A discrete optimization algorithm, introduced in and often used as a first baseline.</p>
</dd></dl>

<dl class="py function">
<dt id="nevergrad.optimization.optimizerlib.learn_on_k_best">
<code class="sig-prename descclassname">nevergrad.optimization.optimizerlib.</code><code class="sig-name descname">learn_on_k_best</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">archive</span><span class="p">:</span> <span class="n">nevergrad.optimization.utils.Archive<span class="p">[</span>nevergrad.optimization.utils.MultiValue<span class="p">]</span></span></em>, <em class="sig-param"><span class="n">k</span><span class="p">:</span> <span class="n">int</span></em><span class="sig-paren">)</span> &#x2192; Union<span class="p">[</span>Tuple<span class="p">[</span>float<span class="p">, </span><span class="p">…</span><span class="p">]</span><span class="p">, </span>List<span class="p">[</span>float<span class="p">]</span><span class="p">, </span>numpy.ndarray<span class="p">]</span><a class="headerlink" href="#nevergrad.optimization.optimizerlib.learn_on_k_best" title="Permalink to this definition">¶</a></dt>
<dd><p>Approximate optimum learnt from the k best.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>archive</strong> (<em>utils.Archive</em><em>[</em><em>utils.Value</em><em>]</em>) – </p>
</dd>
</dl>
</dd></dl>

</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="parametrization_ref.html" class="btn btn-neutral float-right" title="Parametrization API reference" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="machinelearning.html" class="btn btn-neutral float-left" title="Examples - Nevergrad for machine learning" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2019, Facebook AI Research.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>